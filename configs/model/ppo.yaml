name: ppo

device: ${device}

# parameters to instantiate the agent
agent:
  _target_: agents.policy_gradient.Agent_PPO
  learning_rate_pi: 1e-5
  learning_rate_vf: 1e-4
  gamma: 0.99
  steps_per_epoch: 4000
  clip_ratio: 0.2
  train_pi_iters: 80
  train_vf_iters: 80
  lam: 0.97
  target_kl: 0.01
  max_kl_ratio: 1.5
  use_random_action_noise: True
  random_action_noise_size: 0.2
  optimiser: "adam" # adam/adamW/RMSProp
  use_kl_penalty: False
  use_entropy_regularisation: False
  kl_penalty_coefficient: 0.2
  entropy_coefficient:  1e-4
  adam_beta1: 0.9
  adam_beta2: 0.999
  grad_clamp_value: null
  rngseed: 123

# parameters to instantiate the network
network:
  _target_: agents.policy_gradient.MLPActorCriticPG
  obs_dim: ???
  act_dim: ???
  hidden_sizes:
    - 64
    - 64
  continous_actions: True

# parameters to instantiate the trainer
trainer:
  _target_: trainers.base_trainer.Trainer
  num_episodes: 10000
  test_freq: 1000
  save_freq: 1000
  use_curriculum: False
  rngseed: 123
  log_level: 1
